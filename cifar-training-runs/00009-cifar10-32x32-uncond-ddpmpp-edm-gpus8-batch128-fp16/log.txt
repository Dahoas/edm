Loading datasets...
Constructing network...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
/mnt/nvme/home/kevinrojas1499/repos/edm/training/networks.py:174: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484806139/work/aten/src/ATen/EmptyTensor.cpp:31.)
  x_ft = torch.fft.rfft2(x)

EDMPrecond            Parameters  Buffers  Output shape       Datatype
---                   ---         ---      ---                ---     
model.map_noise       -           -        [16, 128]          float32 
model.map_layer0      66048       -        [16, 512]          float32 
model.map_layer1      262656      -        [16, 512]          float32 
model.enc.0_conv      224768      -        [16, 128, 32, 32]  float16 
model.enc.0_block0    19235712    -        [16, 128, 32, 32]  float16 
model.enc.0_block1    19235712    -        [16, 128, 32, 32]  float16 
model.enc.0_block2    19235712    -        [16, 128, 32, 32]  float16 
model.enc.0_block3    19235712    -        [16, 128, 32, 32]  float16 
model.enc.1_down      5620736     8        [16, 128, 16, 16]  float16 
model.enc.1_block0    16779008    -        [16, 256, 16, 16]  float16 
model.enc.1_block1    22284032    -        [16, 256, 16, 16]  float16 
model.enc.1_block2    22284032    -        [16, 256, 16, 16]  float16 
model.enc.1_block3    22284032    -        [16, 256, 16, 16]  float16 
model.enc.2_down      7669760     8        [16, 256, 8, 8]    float16 
model.enc.2_block0    7867648     -        [16, 256, 8, 8]    float16 
model.enc.2_block1    7867648     -        [16, 256, 8, 8]    float16 
model.enc.2_block2    7867648     -        [16, 256, 8, 8]    float16 
model.enc.2_block3    7867648     -        [16, 256, 8, 8]    float16 
model.enc.3_down      1378304     8        [16, 256, 4, 4]    float16 
model.enc.3_block0    1312512     -        [16, 256, 4, 4]    float16 
model.enc.3_block1    1312512     -        [16, 256, 4, 4]    float16 
model.enc.3_block2    1312512     -        [16, 256, 4, 4]    float16 
model.enc.3_block3    1312512     -        [16, 256, 4, 4]    float16 
model.dec.3_in0       1576192     -        [16, 256, 4, 4]    float16 
model.dec.3_in1       1312512     -        [16, 256, 4, 4]    float16 
model.dec.3_block0    2034176     -        [16, 256, 4, 4]    float16 
model.dec.3_block1    2034176     -        [16, 256, 4, 4]    float16 
model.dec.3_block2    2034176     -        [16, 256, 4, 4]    float16 
model.dec.3_block3    2034176     -        [16, 256, 4, 4]    float16 
model.dec.3_block4    2034176     -        [16, 256, 4, 4]    float16 
model.dec.2_up        1378304     8        [16, 256, 8, 8]    float16 
model.dec.2_block0    11471360    -        [16, 256, 8, 8]    float16 
model.dec.2_block1    11471360    -        [16, 256, 8, 8]    float16 
model.dec.2_block2    11471360    -        [16, 256, 8, 8]    float16 
model.dec.2_block3    11471360    -        [16, 256, 8, 8]    float16 
model.dec.2_block4    11735040    -        [16, 256, 8, 8]    float16 
model.dec.1_up        7669760     8        [16, 256, 16, 16]  float16 
model.dec.1_block0    33491456    -        [16, 256, 16, 16]  float16 
model.dec.1_block1    33491456    -        [16, 256, 16, 16]  float16 
model.dec.1_block2    33491456    -        [16, 256, 16, 16]  float16 
model.dec.1_block3    33491456    -        [16, 256, 16, 16]  float16 
model.dec.1_block4    27920640    -        [16, 256, 16, 16]  float16 
model.dec.0_up        22349824    8        [16, 256, 32, 32]  float16 
model.dec.0_block0    38454784    -        [16, 128, 32, 32]  float16 
model.dec.0_block1    28853504    -        [16, 128, 32, 32]  float16 
model.dec.0_block2    28853504    -        [16, 128, 32, 32]  float16 
model.dec.0_block3    28853504    -        [16, 128, 32, 32]  float16 
model.dec.0_block4    28853504    -        [16, 128, 32, 32]  float16 
model.dec.0_aux_norm  256         -        [16, 128, 32, 32]  float16 
model.dec.0_aux_conv  224643      -        [16, 3, 32, 32]    float16 
model                 1152        -        [16, 3, 32, 32]    float16 
<top-level>           -           -        [16, 3, 32, 32]    float32 
---                   ---         ---      ---                ---     
Total                 630576131   48       -                  -       

Setting up optimizer...
Training for 200000 kimg...

tick 0     kimg 0.1       time 20s          sec/tick 3.0     sec/kimg 23.63   maintenance 16.7   cpumem 3.48   gpumem 14.28  reserved 14.67 
tick 1     kimg 50.2      time 1m 52s       sec/tick 88.4    sec/kimg 1.77    maintenance 4.4    cpumem 5.81   gpumem 16.63  reserved 17.10 

Aborted!
