Loading dataset...
Constructing network...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Input has shape: torch.Size([16, 3, 32, 32])
Has nan: False
/mnt/nvme/home/alex/repos/diffusion/edm/training/networks.py:166: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31.)
  x_ft = torch.fft.rfft2(x)
Out shape at block 0_conv: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 0_block0: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 0_block1: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 0_block2: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 0_block3: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 1_down: torch.Size([16, 128, 16, 16])
Has nan: False
Out shape at block 1_block0: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 1_block1: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 1_block2: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 1_block3: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 2_down: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block0: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block1: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block2: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block3: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 3_down: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block0: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block1: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block2: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block3: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_in0: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_in1: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block0: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block1: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block2: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block3: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block4: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 2_up: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block0: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block1: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block2: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block3: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block4: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 1_up: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 1_block0: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 1_block1: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 1_block2: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 1_block3: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 1_block4: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 0_up: torch.Size([16, 256, 32, 32])
Has nan: False
Out shape at block 0_block0: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 0_block1: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 0_block2: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 0_block3: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 0_block4: torch.Size([16, 128, 32, 32])
Has nan: False

EDMPrecond            Parameters  Buffers  Output shape       Datatype
---                   ---         ---      ---                ---     
model.map_noise       -           -        [16, 128]          float32 
model.map_layer0      66048       -        [16, 512]          float32 
model.map_layer1      262656      -        [16, 512]          float32 
model.enc.0_conv      58880       -        [16, 128, 32, 32]  float16 
model.enc.0_block0    5079936     -        [16, 128, 32, 32]  float16 
model.enc.0_block1    5079936     -        [16, 128, 32, 32]  float16 
model.enc.0_block2    5079936     -        [16, 128, 32, 32]  float16 
model.enc.0_block3    5079936     -        [16, 128, 32, 32]  float16 
model.enc.1_down      1688576     8        [16, 128, 16, 16]  float16 
model.enc.1_block0    4982528     -        [16, 256, 16, 16]  float16 
model.enc.1_block1    6555392     -        [16, 256, 16, 16]  float16 
model.enc.1_block2    6555392     -        [16, 256, 16, 16]  float16 
model.enc.1_block3    6555392     -        [16, 256, 16, 16]  float16 
model.enc.2_down      6621184     8        [16, 256, 8, 8]    float16 
model.enc.2_block0    6819072     -        [16, 256, 8, 8]    float16 
model.enc.2_block1    6819072     -        [16, 256, 8, 8]    float16 
model.enc.2_block2    6819072     -        [16, 256, 8, 8]    float16 
model.enc.2_block3    6819072     -        [16, 256, 8, 8]    float16 
model.enc.3_down      1378304     8        [16, 256, 4, 4]    float16 
model.enc.3_block0    1312512     -        [16, 256, 4, 4]    float16 
model.enc.3_block1    1312512     -        [16, 256, 4, 4]    float16 
model.enc.3_block2    1312512     -        [16, 256, 4, 4]    float16 
model.enc.3_block3    1312512     -        [16, 256, 4, 4]    float16 
model.dec.3_in0       1576192     -        [16, 256, 4, 4]    float16 
model.dec.3_in1       1312512     -        [16, 256, 4, 4]    float16 
model.dec.3_block0    2034176     -        [16, 256, 4, 4]    float16 
model.dec.3_block1    2034176     -        [16, 256, 4, 4]    float16 
model.dec.3_block2    2034176     -        [16, 256, 4, 4]    float16 
model.dec.3_block3    2034176     -        [16, 256, 4, 4]    float16 
model.dec.3_block4    2034176     -        [16, 256, 4, 4]    float16 
model.dec.2_up        1378304     8        [16, 256, 8, 8]    float16 
model.dec.2_block0    9898496     -        [16, 256, 8, 8]    float16 
model.dec.2_block1    9898496     -        [16, 256, 8, 8]    float16 
model.dec.2_block2    9898496     -        [16, 256, 8, 8]    float16 
model.dec.2_block3    9898496     -        [16, 256, 8, 8]    float16 
model.dec.2_block4    10162176    -        [16, 256, 8, 8]    float16 
model.dec.1_up        6621184     8        [16, 256, 16, 16]  float16 
model.dec.1_block0    9898496     -        [16, 256, 16, 16]  float16 
model.dec.1_block1    9898496     -        [16, 256, 16, 16]  float16 
model.dec.1_block2    9898496     -        [16, 256, 16, 16]  float16 
model.dec.1_block3    9898496     -        [16, 256, 16, 16]  float16 
model.dec.1_block4    8259840     -        [16, 256, 16, 16]  float16 
model.dec.0_up        6621184     8        [16, 256, 32, 32]  float16 
model.dec.0_block0    10143232    -        [16, 128, 32, 32]  float16 
model.dec.0_block1    7619840     -        [16, 128, 32, 32]  float16 
model.dec.0_block2    7619840     -        [16, 128, 32, 32]  float16 
model.dec.0_block3    7619840     -        [16, 128, 32, 32]  float16 
model.dec.0_block4    7619840     -        [16, 128, 32, 32]  float16 
model.dec.0_aux_norm  256         -        [16, 128, 32, 32]  float16 
model.dec.0_aux_conv  58755       -        [16, 3, 32, 32]    float16 
model                 1152        -        [16, 3, 32, 32]    float16 
<top-level>           -           -        [16, 3, 32, 32]    float32 
---                   ---         ---      ---                ---     
Total                 253543427   48       -                  -       

Setting up optimizer...
Training for 200000 kimg...

Input has shape: torch.Size([16, 3, 32, 32])
Has nan: False
Out shape at block 0_conv: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 0_block0: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 0_block1: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 0_block2: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 0_block3: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 1_down: torch.Size([16, 128, 16, 16])
Has nan: False
Out shape at block 1_block0: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 1_block1: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 1_block2: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 1_block3: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 2_down: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block0: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block1: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block2: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block3: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 3_down: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block0: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block1: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block2: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block3: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_in0: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_in1: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block0: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block1: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block2: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block3: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 3_block4: torch.Size([16, 256, 4, 4])
Has nan: False
Out shape at block 2_up: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block0: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block1: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block2: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block3: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 2_block4: torch.Size([16, 256, 8, 8])
Has nan: False
Out shape at block 1_up: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 1_block0: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 1_block1: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 1_block2: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 1_block3: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 1_block4: torch.Size([16, 256, 16, 16])
Has nan: False
Out shape at block 0_up: torch.Size([16, 256, 32, 32])
Has nan: False
Out shape at block 0_block0: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 0_block1: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 0_block2: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 0_block3: torch.Size([16, 128, 32, 32])
Has nan: False
Out shape at block 0_block4: torch.Size([16, 128, 32, 32])
Has nan: False
tensor([[[[ 9.4427e-01,  4.9604e-01,  4.6403e-01,  ...,  6.0925e-01,
            5.8776e-01,  6.1687e-01],
          [ 4.0352e-01,  5.7495e-01,  6.3700e-01,  ...,  2.7330e-01,
            1.6317e-01,  3.0658e-01],
          [ 4.6443e-01,  3.7257e-01,  3.3617e-01,  ...,  1.7642e-01,
            1.6427e-02, -5.0656e-02],
          ...,
          [ 5.0704e-01,  3.8238e-02,  3.6253e-01,  ...,  5.7767e-01,
            3.2508e-01,  4.5092e-01],
          [ 6.5076e-01,  5.4157e-01,  7.6508e-01,  ...,  2.9743e-01,
            3.9900e-01,  3.6018e-01],
          [ 1.0347e-01,  7.9492e-01,  2.8584e-01,  ..., -8.6512e-02,
            1.3231e-01,  3.9466e-01]],

         [[ 7.3198e-01,  5.5775e-01,  5.6021e-01,  ...,  7.5053e-01,
            6.4953e-01,  2.9731e-01],
          [ 5.6737e-01,  4.5229e-01,  4.9344e-01,  ..., -6.0530e-02,
            6.3312e-02, -8.3810e-02],
          [ 4.4759e-01,  2.5691e-01,  3.8328e-01,  ..., -4.2041e-02,
            3.3910e-01,  1.4177e-01],
          ...,
          [ 4.8941e-02,  3.6958e-01,  2.4625e-01,  ...,  4.2089e-01,
            6.9070e-02,  3.8816e-01],
          [ 1.3138e-01,  9.2997e-02,  3.1632e-01,  ...,  6.8704e-01,
            5.6600e-01,  5.3479e-01],
          [ 3.7118e-01,  1.0051e-02,  5.2825e-01,  ...,  1.8292e-01,
            1.4245e-01,  5.2476e-01]],

         [[ 5.2663e-01,  5.1689e-01,  5.4901e-01,  ...,  5.4318e-01,
            6.1546e-01,  1.8431e-01],
          [ 8.9151e-01,  5.4183e-01,  4.9764e-01,  ...,  2.9294e-01,
            4.6497e-01,  2.2115e-01],
          [ 7.9855e-01,  4.7063e-01,  3.8870e-01,  ...,  4.0074e-01,
           -3.1779e-01, -1.5473e-01],
          ...,
          [ 2.3444e-01,  7.5356e-02, -7.5668e-02,  ...,  8.6542e-02,
            4.9010e-01, -9.4658e-03],
          [-3.7802e-02,  1.8888e-01, -7.3122e-03,  ...,  3.6464e-02,
            2.7853e-01, -1.1774e-01],
          [ 1.7627e-01,  1.3435e-01,  3.2025e-01,  ..., -5.3216e-02,
            1.5013e-01,  7.2851e-01]]],


        [[[ 8.8027e-02, -1.6315e-01, -4.1766e-02,  ..., -1.2010e-01,
            1.6221e-01,  3.7006e-01],
          [ 5.7789e-02, -1.0777e-02,  7.5465e-02,  ..., -1.9504e-01,
           -4.2564e-01, -6.5004e-02],
          [ 8.9523e-02,  2.4995e-01,  5.2413e-01,  ...,  1.1036e-01,
            2.9158e-02,  1.3829e-01],
          ...,
          [-4.6174e-01, -4.1906e-02,  1.9835e-01,  ..., -3.3044e-01,
            3.9245e-02, -4.5503e-01],
          [ 2.4546e-01, -4.0073e-02,  3.7320e-01,  ...,  2.4445e-02,
           -4.9504e-01, -7.4099e-02],
          [ 2.3297e-01, -1.2459e-01, -2.4898e-02,  ...,  9.6336e-02,
           -1.0301e-02, -4.7679e-03]],

         [[-3.3390e-01, -9.7936e-02, -3.4878e-01,  ...,  2.2664e-01,
            4.2901e-02,  4.9301e-02],
          [ 1.5615e-02,  1.3628e-01,  8.5002e-02,  ...,  4.0378e-01,
           -3.8523e-01,  9.2880e-02],
          [-4.2124e-02,  8.1081e-02,  3.3788e-01,  ...,  2.0495e-01,
           -4.1486e-01, -2.5573e-01],
          ...,
          [ 9.1743e-02,  1.2202e-01,  2.8097e-01,  ...,  6.0816e-02,
           -3.5351e-01,  1.9317e-02],
          [ 1.8488e-01, -1.9366e-01,  5.1518e-01,  ..., -1.7905e-01,
           -3.0980e-01, -4.1350e-01],
          [-2.9782e-01,  4.5107e-01, -3.7851e-02,  ..., -3.8602e-02,
           -2.1162e-01,  2.2070e-01]],

         [[ 4.7518e-02, -9.1297e-02,  2.1793e-01,  ...,  1.3752e-01,
           -2.3661e-01,  1.5965e-01],
          [ 3.4350e-01, -2.3956e-01,  1.0296e-01,  ...,  3.1040e-01,
            1.8495e-01, -4.7844e-02],
          [-2.8569e-01,  2.6250e-01, -1.0306e-01,  ..., -4.0706e-01,
           -2.6948e-01, -7.1110e-02],
          ...,
          [-1.3523e-01, -1.8224e-01, -2.6997e-01,  ..., -1.4346e-02,
            8.6885e-02,  4.1879e-01],
          [ 1.9037e-01,  3.5113e-02,  1.4561e-01,  ..., -2.3030e-01,
           -1.3324e-01, -5.6702e-01],
          [-3.7308e-01,  1.9849e-01, -6.0271e-02,  ..., -7.4952e-02,
           -8.1761e-02, -1.0664e-02]]],


        [[[ 1.0169e+00,  1.0195e+00,  9.8800e-01,  ...,  1.0266e+00,
            9.9242e-01,  9.2097e-01],
          [ 9.4032e-01,  9.8367e-01,  1.0009e+00,  ...,  9.8600e-01,
            9.7448e-01,  9.3231e-01],
          [ 9.9216e-01,  9.5586e-01,  9.4625e-01,  ...,  9.7379e-01,
            9.1602e-01,  9.3779e-01],
          ...,
          [ 9.8325e-01,  1.0122e+00,  1.0217e+00,  ...,  9.9922e-01,
            1.0184e+00,  1.0566e+00],
          [ 9.9625e-01,  9.9872e-01,  9.4798e-01,  ...,  9.3833e-01,
            9.7062e-01,  9.5462e-01],
          [ 1.0217e+00,  1.0316e+00,  1.0098e+00,  ...,  9.5536e-01,
            9.4303e-01,  9.6350e-01]],

         [[ 9.9908e-01,  9.7676e-01,  9.4160e-01,  ...,  9.7162e-01,
            9.8563e-01,  9.5762e-01],
          [ 9.9414e-01,  9.8447e-01,  1.0280e+00,  ...,  9.7925e-01,
            9.4287e-01,  1.0190e+00],
          [ 9.7037e-01,  9.6076e-01,  8.8518e-01,  ...,  9.7786e-01,
            9.4200e-01,  9.2516e-01],
          ...,
          [ 1.0108e+00,  9.6829e-01,  9.7981e-01,  ...,  1.0160e+00,
            9.7389e-01,  1.0138e+00],
          [ 9.7674e-01,  9.8777e-01,  1.0144e+00,  ...,  9.5606e-01,
            1.0044e+00,  9.5641e-01],
          [ 9.7814e-01,  1.0280e+00,  1.0157e+00,  ...,  1.0485e+00,
            9.7519e-01,  9.8291e-01]],

         [[ 1.0050e+00,  9.9167e-01,  9.6720e-01,  ...,  1.0018e+00,
            9.6198e-01,  9.6406e-01],
          [ 1.0071e+00,  9.8684e-01,  9.5661e-01,  ...,  9.9321e-01,
            9.9164e-01,  9.7543e-01],
          [ 9.2946e-01,  9.9309e-01,  9.6149e-01,  ...,  1.0161e+00,
            9.8400e-01,  9.9928e-01],
          ...,
          [ 1.0143e+00,  9.6172e-01,  9.6898e-01,  ...,  9.6456e-01,
            9.9374e-01,  9.9414e-01],
          [ 9.7356e-01,  1.0143e+00,  1.0049e+00,  ...,  9.7722e-01,
            9.8227e-01,  1.0200e+00],
          [ 9.9132e-01,  9.7414e-01,  9.7068e-01,  ...,  9.5390e-01,
            9.2676e-01,  9.5452e-01]]],


        ...,


        [[[-1.7435e-01, -2.3413e-01, -2.3409e-01,  ..., -3.2768e-01,
           -2.5559e-01, -4.1090e-01],
          [-1.7830e-01, -2.4013e-01, -1.2879e-01,  ..., -2.0699e-02,
            5.5501e-02, -3.7773e-01],
          [-1.7858e-01, -3.1043e-01,  1.4674e-01,  ..., -1.0564e-01,
            1.4264e-01, -2.4233e-01],
          ...,
          [-2.4377e-01, -3.9304e-01, -3.3436e-01,  ..., -2.7813e-01,
           -3.8056e-01, -4.2758e-01],
          [-3.0107e-01, -5.0660e-01, -4.8839e-01,  ..., -5.1559e-01,
           -6.6180e-02, -6.0423e-01],
          [-6.3307e-01, -6.9000e-01, -6.3814e-01,  ..., -8.0612e-01,
           -6.0454e-01, -5.9199e-01]],

         [[-2.3431e-01, -5.4716e-01, -6.5425e-02,  ..., -1.8929e-01,
           -2.0020e-01, -4.1083e-01],
          [-1.4204e-01, -7.3863e-02, -1.3966e-01,  ...,  1.0039e-01,
           -8.8200e-02,  3.1123e-02],
          [ 1.1350e-02, -6.3718e-02,  4.1114e-02,  ...,  2.3788e-01,
           -3.1704e-01, -1.0588e-01],
          ...,
          [-1.7491e-01, -4.7824e-01, -3.2990e-01,  ..., -4.5353e-01,
           -1.9770e-01, -3.6774e-01],
          [-2.6552e-01,  4.2506e-04, -4.3592e-01,  ..., -1.9876e-01,
           -4.1516e-01, -5.5775e-01],
          [-4.4808e-01, -3.6168e-01, -9.1126e-01,  ..., -5.5555e-01,
           -5.0629e-01, -7.5498e-01]],

         [[-2.5209e-01,  1.3939e-01, -5.0560e-02,  ..., -2.1907e-01,
            2.9007e-01,  1.3821e-01],
          [ 1.4566e-02,  1.6348e-02, -2.4146e-01,  ...,  1.6905e-01,
            3.1593e-02, -6.8611e-02],
          [-1.3474e-02,  1.4696e-01,  2.6307e-01,  ...,  5.5548e-01,
            8.4593e-02, -7.0349e-02],
          ...,
          [ 2.4129e-02, -3.1023e-01, -3.3291e-01,  ..., -4.8042e-01,
           -1.3337e-01, -2.1556e-01],
          [-2.9396e-01, -5.4352e-01, -2.4124e-01,  ..., -4.7394e-01,
           -6.2478e-01, -5.2921e-01],
          [-6.5185e-01, -6.3709e-01, -5.3344e-01,  ..., -6.7857e-01,
           -5.0379e-01, -7.9893e-01]]],


        [[[-7.5631e-01, -4.1436e-01, -3.0502e-01,  ..., -4.7259e-01,
           -2.7021e-01, -6.3936e-01],
          [-3.9257e-01, -3.9929e-01, -3.5454e-01,  ..., -5.0942e-01,
           -4.2995e-01, -5.2429e-01],
          [-3.1159e-01, -6.1471e-01, -3.6108e-01,  ..., -3.7025e-01,
           -6.2707e-01, -3.4333e-01],
          ...,
          [-6.4519e-01, -4.3424e-01, -8.5095e-01,  ..., -3.0372e-01,
           -5.3181e-01, -4.3609e-01],
          [-4.1531e-01, -5.2240e-01, -7.9602e-01,  ..., -4.4359e-02,
           -3.0949e-01, -2.0416e-01],
          [-2.5582e-01, -4.3455e-01, -1.0142e-01,  ...,  6.2918e-02,
           -3.4794e-01, -5.1520e-01]],

         [[-6.4936e-01, -5.0961e-01, -2.7802e-01,  ..., -3.5216e-01,
           -4.9239e-01, -4.5050e-01],
          [-8.0158e-01, -3.5682e-01, -8.2677e-01,  ..., -5.8753e-01,
           -3.8973e-01, -8.5186e-01],
          [-6.5818e-01, -5.8760e-01, -3.9437e-01,  ..., -2.7225e-01,
           -7.3959e-01, -4.7744e-01],
          ...,
          [-5.2103e-01, -6.4821e-01, -6.6051e-01,  ..., -1.7748e-01,
           -1.7285e-01, -2.0448e-01],
          [-8.2452e-01, -7.1590e-01, -7.7755e-01,  ..., -1.5614e-01,
           -2.6813e-01, -8.5364e-02],
          [-6.8055e-01, -4.0479e-01, -5.0928e-01,  ..., -1.9263e-01,
           -2.6752e-01, -5.1698e-01]],

         [[-7.4047e-01, -7.9741e-01, -6.6682e-01,  ..., -5.4237e-01,
           -1.9630e-01, -4.9928e-01],
          [-3.3704e-01, -7.4433e-01, -7.0875e-01,  ...,  6.3259e-02,
           -6.6336e-01, -4.0991e-01],
          [-4.4537e-01, -7.4144e-01, -2.8550e-01,  ..., -4.1165e-01,
           -3.9360e-01, -2.4031e-01],
          ...,
          [-3.3914e-01, -5.7738e-01, -6.7151e-01,  ..., -7.0792e-01,
           -3.6730e-01, -2.6951e-01],
          [-6.1170e-01, -8.9042e-01, -6.6576e-01,  ..., -9.7260e-02,
           -5.0742e-01, -2.9940e-01],
          [-5.6811e-01, -5.4060e-01, -8.5898e-01,  ...,  1.7262e-01,
           -2.5705e-01, -2.3780e-01]]],


        [[[ 1.2679e+00,  1.1086e+00,  1.0947e+00,  ...,  6.9004e-01,
            8.5966e-01,  5.8175e-01],
          [ 5.6936e-01,  5.0620e-01,  6.1337e-01,  ...,  8.7726e-01,
            6.2365e-01,  9.4760e-01],
          [ 3.7868e-01,  7.6958e-01,  5.0311e-01,  ...,  1.1047e+00,
            8.4649e-01,  9.7240e-01],
          ...,
          [ 8.1473e-01,  7.3199e-01,  5.2466e-01,  ...,  7.8766e-01,
            4.4072e-01,  3.6449e-01],
          [ 7.4116e-01,  9.7921e-01,  6.6076e-01,  ...,  9.2178e-01,
            3.7422e-01,  7.4090e-01],
          [ 6.5427e-01,  6.7816e-01,  7.3437e-01,  ...,  6.6880e-01,
            6.9483e-01,  1.1212e+00]],

         [[ 9.0764e-01,  9.7748e-01,  4.3343e-01,  ...,  6.4912e-01,
            1.1806e+00,  6.5763e-01],
          [ 6.4161e-01,  8.6221e-01,  9.4701e-01,  ...,  6.5127e-01,
            7.9140e-01,  6.1749e-01],
          [ 5.3400e-01,  6.7475e-01,  3.7199e-01,  ...,  8.6366e-01,
            7.1033e-01,  1.1797e+00],
          ...,
          [ 8.1724e-01,  3.8807e-01,  4.5075e-01,  ...,  5.9925e-01,
            6.2488e-01,  1.0123e+00],
          [ 4.6036e-01,  5.9951e-01,  1.8830e-01,  ...,  5.1995e-01,
            8.4274e-01,  1.0987e+00],
          [-9.1643e-02,  3.4028e-01,  7.1739e-01,  ...,  6.7229e-01,
            9.7636e-01,  3.8897e-01]],

         [[ 8.0865e-01,  1.0304e+00,  1.0072e+00,  ...,  9.4633e-01,
            1.1618e+00,  6.1722e-01],
          [ 3.1910e-01,  4.6942e-01,  1.0651e+00,  ...,  5.9348e-01,
            3.5437e-01,  5.4774e-01],
          [ 5.6473e-01,  1.2318e+00,  1.0059e+00,  ...,  7.4452e-01,
            1.1558e+00,  1.2249e+00],
          ...,
          [ 6.1859e-01,  2.8710e-01,  9.9106e-01,  ...,  9.7131e-01,
            6.9161e-01,  6.3572e-01],
          [ 9.2012e-01,  7.5761e-01,  5.7428e-01,  ...,  7.9728e-01,
            3.7503e-01,  6.9621e-01],
          [ 1.0389e+00,  3.5131e-01,  5.2046e-01,  ...,  1.0501e+00,
            6.9063e-01,  9.5299e-01]]]], device='cuda:0',
       grad_fn=<AddBackward0>)
