Loading dataset...
Constructing network...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
/mnt/nvme/home/alex/repos/diffusion/edm/training/networks.py:166: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31.)
  x_ft = torch.fft.rfft2(x)

EDMPrecond            Parameters  Buffers  Output shape        Datatype
---                   ---         ---      ---                 ---     
model.map_noise       -           -        [1, 128]            float32 
model.map_layer0      66048       -        [1, 512]            float32 
model.map_layer1      262656      -        [1, 512]            float32 
model.enc.0_conv      421376      -        [1, 128, 256, 256]  float16 
model.enc.0_block0    36012928    -        [1, 128, 256, 256]  float16 
model.enc.0_block1    36012928    -        [1, 128, 256, 256]  float16 
model.enc.0_block2    36012928    -        [1, 128, 256, 256]  float16 
model.enc.0_block3    36012928    -        [1, 128, 256, 256]  float16 
model.enc.1_down      9815040     8        [1, 128, 128, 128]  float16 
model.enc.1_block0    29361920    -        [1, 256, 128, 128]  float16 
model.enc.1_block1    39061248    -        [1, 256, 128, 128]  float16 
model.enc.1_block2    39061248    -        [1, 256, 128, 128]  float16 
model.enc.1_block3    39061248    -        [1, 256, 128, 128]  float16 
model.enc.2_down      11864064    8        [1, 256, 64, 64]    float16 
model.enc.2_block0    11798272    -        [1, 256, 64, 64]    float16 
model.enc.2_block1    11798272    -        [1, 256, 64, 64]    float16 
model.enc.2_block2    11798272    -        [1, 256, 64, 64]    float16 
model.enc.2_block3    11798272    -        [1, 256, 64, 64]    float16 
model.enc.3_down      1378304     8        [1, 256, 32, 32]    float16 
model.enc.3_block0    1312512     -        [1, 256, 32, 32]    float16 
model.enc.3_block1    1312512     -        [1, 256, 32, 32]    float16 
model.enc.3_block2    1312512     -        [1, 256, 32, 32]    float16 
model.enc.3_block3    1312512     -        [1, 256, 32, 32]    float16 
model.enc.4_down      1378304     8        [1, 256, 16, 16]    float16 
model.enc.4_block0    1576192     -        [1, 256, 16, 16]    float16 
model.enc.4_block1    1576192     -        [1, 256, 16, 16]    float16 
model.enc.4_block2    1576192     -        [1, 256, 16, 16]    float16 
model.enc.4_block3    1576192     -        [1, 256, 16, 16]    float16 
model.dec.4_in0       1576192     -        [1, 256, 16, 16]    float16 
model.dec.4_in1       1312512     -        [1, 256, 16, 16]    float16 
model.dec.4_block0    2034176     -        [1, 256, 16, 16]    float16 
model.dec.4_block1    2034176     -        [1, 256, 16, 16]    float16 
model.dec.4_block2    2034176     -        [1, 256, 16, 16]    float16 
model.dec.4_block3    2034176     -        [1, 256, 16, 16]    float16 
model.dec.4_block4    2297856     -        [1, 256, 16, 16]    float16 
model.dec.3_up        1378304     8        [1, 256, 32, 32]    float16 
model.dec.3_block0    2034176     -        [1, 256, 32, 32]    float16 
model.dec.3_block1    2034176     -        [1, 256, 32, 32]    float16 
model.dec.3_block2    2034176     -        [1, 256, 32, 32]    float16 
model.dec.3_block3    2034176     -        [1, 256, 32, 32]    float16 
model.dec.3_block4    2034176     -        [1, 256, 32, 32]    float16 
model.dec.2_up        1378304     8        [1, 256, 64, 64]    float16 
model.dec.2_block0    2034176     -        [1, 256, 64, 64]    float16 
model.dec.2_block1    2034176     -        [1, 256, 64, 64]    float16 
model.dec.2_block2    2034176     -        [1, 256, 64, 64]    float16 
model.dec.2_block3    2034176     -        [1, 256, 64, 64]    float16 
model.dec.2_block4    2034176     -        [1, 256, 64, 64]    float16 
model.dec.1_up        39127040    8        [1, 256, 128, 128]  float16 
model.dec.1_block0    58657280    -        [1, 256, 128, 128]  float16 
model.dec.1_block1    58657280    -        [1, 256, 128, 128]  float16 
model.dec.1_block2    58657280    -        [1, 256, 128, 128]  float16 
model.dec.1_block3    58657280    -        [1, 256, 128, 128]  float16 
model.dec.1_block4    48892160    -        [1, 256, 128, 128]  float16 
model.dec.0_up        143984640   8        [1, 256, 256, 256]  float16 
model.dec.0_block0    72009216    -        [1, 128, 256, 256]  float16 
model.dec.0_block1    54019328    -        [1, 128, 256, 256]  float16 
model.dec.0_block2    54019328    -        [1, 128, 256, 256]  float16 
model.dec.0_block3    54019328    -        [1, 128, 256, 256]  float16 
model.dec.0_block4    54019328    -        [1, 128, 256, 256]  float16 
model.dec.0_aux_norm  256         -        [1, 128, 256, 256]  float16 
model.dec.0_aux_conv  421251      -        [1, 3, 256, 256]    float16 
model                 1152        -        [1, 3, 256, 256]    float16 
<top-level>           -           -        [1, 3, 256, 256]    float32 
---                   ---         ---      ---                 ---     
Total                 1166094851  64       -                   -       

Setting up optimizer...
Training for 200000 kimg...

tick 0     kimg 0.0       time 56s          sec/tick 25.1    sec/kimg 3134.68 maintenance 31.3   cpumem 3.30   gpumem 53.13  reserved 70.46 

Aborted!
