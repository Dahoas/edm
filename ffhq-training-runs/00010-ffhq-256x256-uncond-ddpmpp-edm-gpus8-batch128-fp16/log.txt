Loading dataset...
Constructing network...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
/mnt/nvme/home/alex/repos/diffusion/edm/training/networks.py:166: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31.)
  x_ft = torch.fft.rfft2(x)

EDMPrecond            Parameters  Buffers  Output shape         Datatype
---                   ---         ---      ---                  ---     
model.map_noise       -           -        [16, 128]            float32 
model.map_layer0      66048       -        [16, 512]            float32 
model.map_layer1      262656      -        [16, 512]            float32 
model.enc.0_conv      421376      -        [16, 128, 256, 256]  float16 
model.enc.0_block0    36012928    -        [16, 128, 256, 256]  float16 
model.enc.0_block1    36012928    -        [16, 128, 256, 256]  float16 
model.enc.0_block2    36012928    -        [16, 128, 256, 256]  float16 
model.enc.0_block3    36012928    -        [16, 128, 256, 256]  float16 
model.enc.1_down      9815040     8        [16, 128, 128, 128]  float16 
model.enc.1_block0    29361920    -        [16, 256, 128, 128]  float16 
model.enc.1_block1    39061248    -        [16, 256, 128, 128]  float16 
model.enc.1_block2    39061248    -        [16, 256, 128, 128]  float16 
model.enc.1_block3    39061248    -        [16, 256, 128, 128]  float16 
model.enc.2_down      11864064    8        [16, 256, 64, 64]    float16 
model.enc.2_block0    11798272    -        [16, 256, 64, 64]    float16 
model.enc.2_block1    11798272    -        [16, 256, 64, 64]    float16 
model.enc.2_block2    11798272    -        [16, 256, 64, 64]    float16 
model.enc.2_block3    11798272    -        [16, 256, 64, 64]    float16 
model.enc.3_down      1378304     8        [16, 256, 32, 32]    float16 
model.enc.3_block0    1312512     -        [16, 256, 32, 32]    float16 
model.enc.3_block1    1312512     -        [16, 256, 32, 32]    float16 
model.enc.3_block2    1312512     -        [16, 256, 32, 32]    float16 
model.enc.3_block3    1312512     -        [16, 256, 32, 32]    float16 
model.enc.4_down      1378304     8        [16, 256, 16, 16]    float16 
model.enc.4_block0    1576192     -        [16, 256, 16, 16]    float16 
model.enc.4_block1    1576192     -        [16, 256, 16, 16]    float16 
model.enc.4_block2    1576192     -        [16, 256, 16, 16]    float16 
model.enc.4_block3    1576192     -        [16, 256, 16, 16]    float16 
model.dec.4_in0       1576192     -        [16, 256, 16, 16]    float16 
model.dec.4_in1       1312512     -        [16, 256, 16, 16]    float16 
model.dec.4_block0    2034176     -        [16, 256, 16, 16]    float16 
model.dec.4_block1    2034176     -        [16, 256, 16, 16]    float16 
model.dec.4_block2    2034176     -        [16, 256, 16, 16]    float16 
model.dec.4_block3    2034176     -        [16, 256, 16, 16]    float16 
model.dec.4_block4    2297856     -        [16, 256, 16, 16]    float16 
model.dec.3_up        1378304     8        [16, 256, 32, 32]    float16 
model.dec.3_block0    2034176     -        [16, 256, 32, 32]    float16 
model.dec.3_block1    2034176     -        [16, 256, 32, 32]    float16 
model.dec.3_block2    2034176     -        [16, 256, 32, 32]    float16 
model.dec.3_block3    2034176     -        [16, 256, 32, 32]    float16 
model.dec.3_block4    2034176     -        [16, 256, 32, 32]    float16 
model.dec.2_up        1378304     8        [16, 256, 64, 64]    float16 
model.dec.2_block0    2034176     -        [16, 256, 64, 64]    float16 
model.dec.2_block1    2034176     -        [16, 256, 64, 64]    float16 
model.dec.2_block2    2034176     -        [16, 256, 64, 64]    float16 
model.dec.2_block3    2034176     -        [16, 256, 64, 64]    float16 
model.dec.2_block4    2034176     -        [16, 256, 64, 64]    float16 
model.dec.1_up        39127040    8        [16, 256, 128, 128]  float16 
model.dec.1_block0    58657280    -        [16, 256, 128, 128]  float16 
model.dec.1_block1    58657280    -        [16, 256, 128, 128]  float16 
model.dec.1_block2    58657280    -        [16, 256, 128, 128]  float16 
model.dec.1_block3    58657280    -        [16, 256, 128, 128]  float16 
model.dec.1_block4    48892160    -        [16, 256, 128, 128]  float16 
model.dec.0_up        143984640   8        [16, 256, 256, 256]  float16 
model.dec.0_block0    72009216    -        [16, 128, 256, 256]  float16 
model.dec.0_block1    54019328    -        [16, 128, 256, 256]  float16 
model.dec.0_block2    54019328    -        [16, 128, 256, 256]  float16 
model.dec.0_block3    54019328    -        [16, 128, 256, 256]  float16 
model.dec.0_block4    54019328    -        [16, 128, 256, 256]  float16 
model.dec.0_aux_norm  256         -        [16, 128, 256, 256]  float16 
model.dec.0_aux_conv  421251      -        [16, 3, 256, 256]    float16 
model                 1152        -        [16, 3, 256, 256]    float16 
<top-level>           -           -        [16, 3, 256, 256]    float32 
---                   ---         ---      ---                  ---     
Total                 1166094851  64       -                    -       

Setting up optimizer...
Training for 200000 kimg...

tick 0     kimg 0.1       time 56s          sec/tick 24.2    sec/kimg 189.45  maintenance 32.1   cpumem 3.31   gpumem 71.12  reserved 74.84 
tick 1     kimg 50.2      time 10m 27s      sec/tick 563.6   sec/kimg 11.26   maintenance 7.1    cpumem 3.83   gpumem 65.73  reserved 72.69 

Traceback (most recent call last):
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/mnt/nvme/home/alex/repos/diffusion/edm/train.py", line 242, in main
    training_loop.training_loop(**c)
  File "/mnt/nvme/home/alex/repos/diffusion/edm/training/training_loop.py", line 139, in training_loop
    optimizer.step()
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/torch/optim/adam.py", line 234, in step
    adam(params_with_grad,
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/torch/optim/adam.py", line 300, in adam
    func(params,
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/torch/optim/adam.py", line 363, in _single_tensor_adam
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/nvme/home/alex/repos/diffusion/edm/train.py", line 247, in <module>
    main()
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/click/core.py", line 1067, in main
    echo(file=sys.stderr)
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/click/utils.py", line 299, in echo
    file.write(out)  # type: ignore
  File "/mnt/nvme/home/alex/repos/diffusion/edm/dnnlib/util.py", line 90, in write
    self.flush()
  File "/mnt/nvme/home/alex/repos/diffusion/edm/dnnlib/util.py", line 95, in flush
    self.file.flush()
KeyboardInterrupt
