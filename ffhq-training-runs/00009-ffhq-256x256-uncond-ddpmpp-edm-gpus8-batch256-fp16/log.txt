Loading dataset...
Constructing network...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
Initializing fourier layer...
/mnt/nvme/home/alex/repos/diffusion/edm/training/networks.py:166: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ../aten/src/ATen/EmptyTensor.cpp:31.)
  x_ft = torch.fft.rfft2(x)

EDMPrecond            Parameters  Buffers  Output shape         Datatype
---                   ---         ---      ---                  ---     
model.map_noise       -           -        [32, 128]            float32 
model.map_layer0      66048       -        [32, 512]            float32 
model.map_layer1      262656      -        [32, 512]            float32 
model.enc.0_conv      421376      -        [32, 128, 256, 256]  float16 
model.enc.0_block0    36012928    -        [32, 128, 256, 256]  float16 
model.enc.0_block1    36012928    -        [32, 128, 256, 256]  float16 
model.enc.0_block2    36012928    -        [32, 128, 256, 256]  float16 
model.enc.0_block3    36012928    -        [32, 128, 256, 256]  float16 
model.enc.1_down      9815040     8        [32, 128, 128, 128]  float16 
model.enc.1_block0    29361920    -        [32, 256, 128, 128]  float16 
model.enc.1_block1    39061248    -        [32, 256, 128, 128]  float16 
model.enc.1_block2    39061248    -        [32, 256, 128, 128]  float16 
model.enc.1_block3    39061248    -        [32, 256, 128, 128]  float16 
model.enc.2_down      11864064    8        [32, 256, 64, 64]    float16 
model.enc.2_block0    11798272    -        [32, 256, 64, 64]    float16 
model.enc.2_block1    11798272    -        [32, 256, 64, 64]    float16 
model.enc.2_block2    11798272    -        [32, 256, 64, 64]    float16 
model.enc.2_block3    11798272    -        [32, 256, 64, 64]    float16 
model.enc.3_down      1378304     8        [32, 256, 32, 32]    float16 
model.enc.3_block0    1312512     -        [32, 256, 32, 32]    float16 
model.enc.3_block1    1312512     -        [32, 256, 32, 32]    float16 
model.enc.3_block2    1312512     -        [32, 256, 32, 32]    float16 
model.enc.3_block3    1312512     -        [32, 256, 32, 32]    float16 
model.enc.4_down      1378304     8        [32, 256, 16, 16]    float16 
model.enc.4_block0    1576192     -        [32, 256, 16, 16]    float16 
model.enc.4_block1    1576192     -        [32, 256, 16, 16]    float16 
model.enc.4_block2    1576192     -        [32, 256, 16, 16]    float16 
model.enc.4_block3    1576192     -        [32, 256, 16, 16]    float16 
model.dec.4_in0       1576192     -        [32, 256, 16, 16]    float16 
model.dec.4_in1       1312512     -        [32, 256, 16, 16]    float16 
model.dec.4_block0    2034176     -        [32, 256, 16, 16]    float16 
model.dec.4_block1    2034176     -        [32, 256, 16, 16]    float16 
model.dec.4_block2    2034176     -        [32, 256, 16, 16]    float16 
model.dec.4_block3    2034176     -        [32, 256, 16, 16]    float16 
model.dec.4_block4    2297856     -        [32, 256, 16, 16]    float16 
model.dec.3_up        1378304     8        [32, 256, 32, 32]    float16 
model.dec.3_block0    2034176     -        [32, 256, 32, 32]    float16 
model.dec.3_block1    2034176     -        [32, 256, 32, 32]    float16 
model.dec.3_block2    2034176     -        [32, 256, 32, 32]    float16 
model.dec.3_block3    2034176     -        [32, 256, 32, 32]    float16 
model.dec.3_block4    2034176     -        [32, 256, 32, 32]    float16 
model.dec.2_up        1378304     8        [32, 256, 64, 64]    float16 
model.dec.2_block0    2034176     -        [32, 256, 64, 64]    float16 
model.dec.2_block1    2034176     -        [32, 256, 64, 64]    float16 
model.dec.2_block2    2034176     -        [32, 256, 64, 64]    float16 
model.dec.2_block3    2034176     -        [32, 256, 64, 64]    float16 
model.dec.2_block4    2034176     -        [32, 256, 64, 64]    float16 
model.dec.1_up        39127040    8        [32, 256, 128, 128]  float16 
model.dec.1_block0    58657280    -        [32, 256, 128, 128]  float16 
model.dec.1_block1    58657280    -        [32, 256, 128, 128]  float16 
model.dec.1_block2    58657280    -        [32, 256, 128, 128]  float16 
model.dec.1_block3    58657280    -        [32, 256, 128, 128]  float16 
model.dec.1_block4    48892160    -        [32, 256, 128, 128]  float16 
model.dec.0_up        143984640   8        [32, 256, 256, 256]  float16 
model.dec.0_block0    72009216    -        [32, 128, 256, 256]  float16 
model.dec.0_block1    54019328    -        [32, 128, 256, 256]  float16 
model.dec.0_block2    54019328    -        [32, 128, 256, 256]  float16 
model.dec.0_block3    54019328    -        [32, 128, 256, 256]  float16 
model.dec.0_block4    54019328    -        [32, 128, 256, 256]  float16 
model.dec.0_aux_norm  256         -        [32, 128, 256, 256]  float16 
model.dec.0_aux_conv  421251      -        [32, 3, 256, 256]    float16 
model                 1152        -        [32, 3, 256, 256]    float16 
<top-level>           -           -        [32, 3, 256, 256]    float32 
---                   ---         ---      ---                  ---     
Total                 1166094851  64       -                    -       

Setting up optimizer...
Training for 200000 kimg...

Traceback (most recent call last):
  File "/mnt/nvme/home/alex/repos/diffusion/edm/train.py", line 247, in <module>
    main()
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/mnt/nvme/home/alex/repos/diffusion/edm/train.py", line 242, in main
    training_loop.training_loop(**c)
  File "/mnt/nvme/home/alex/repos/diffusion/edm/training/training_loop.py", line 129, in training_loop
    loss = loss_fn(net=ddp, images=images, labels=labels, augment_pipe=augment_pipe)
  File "/mnt/nvme/home/alex/repos/diffusion/edm/training/loss.py", line 78, in __call__
    D_yn = net(y + n, sigma, labels, augment_labels=augment_labels)
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nvme/home/alex/repos/diffusion/edm/training/networks.py", line 541, in forward
    F_x = self.model((c_in * x).to(dtype), c_noise.flatten(), class_labels=class_labels, **model_kwargs)
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nvme/home/alex/repos/diffusion/edm/training/networks.py", line 498, in forward
    x = block(x, emb)
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nvme/home/alex/repos/diffusion/edm/training/networks.py", line 298, in forward
    x = self.conv0(silu(self.norm0(x)))
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nvme/home/alex/repos/diffusion/edm/training/networks.py", line 206, in forward
    spectral_out = self.spectral_conv(x) if self.use_spectral else 0
  File "/mnt/nvme/home/alex/.envs/diffusion/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/nvme/home/alex/repos/diffusion/edm/training/networks.py", line 166, in forward
    x_ft = torch.fft.rfft2(x)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.02 GiB (GPU 0; 79.18 GiB total capacity; 75.05 GiB already allocated; 870.31 MiB free; 76.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
